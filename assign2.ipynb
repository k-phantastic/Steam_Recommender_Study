{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf5960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0.0\n",
      "2.3.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import ast\n",
    "import pyarrow\n",
    "print(pyarrow.__version__)\n",
    "print(pd.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "from sklearn.decomposition import TruncatedSVD, MiniBatchTruncatedSVD\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c1dab61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bundle_final_price</th>\n",
       "      <th>bundle_url</th>\n",
       "      <th>bundle_price</th>\n",
       "      <th>bundle_name</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>items</th>\n",
       "      <th>bundle_discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$66.46</td>\n",
       "      <td>http://store.steampowered.com/bundle/450/?utm_...</td>\n",
       "      <td>$73.86</td>\n",
       "      <td>Dharker Studio 2015 Complete</td>\n",
       "      <td>450</td>\n",
       "      <td>[{'genre': 'Adventure, Indie, RPG', 'item_id':...</td>\n",
       "      <td>10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$20.15</td>\n",
       "      <td>http://store.steampowered.com/bundle/1473/?utm...</td>\n",
       "      <td>$25.87</td>\n",
       "      <td>Naruto Shippuden Uncut Season 4, Vol. 3</td>\n",
       "      <td>1473</td>\n",
       "      <td>[{'genre': '', 'item_id': '528360', 'discounte...</td>\n",
       "      <td>22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$20.15</td>\n",
       "      <td>http://store.steampowered.com/bundle/1474/?utm...</td>\n",
       "      <td>$25.87</td>\n",
       "      <td>Naruto Shippuden Uncut Season 4, Vol. 4</td>\n",
       "      <td>1474</td>\n",
       "      <td>[{'genre': '', 'item_id': '528380', 'discounte...</td>\n",
       "      <td>22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$15.17</td>\n",
       "      <td>http://store.steampowered.com/bundle/1437/?utm...</td>\n",
       "      <td>$25.35</td>\n",
       "      <td>Pixel Puzzles Ultimate: Starter Kit</td>\n",
       "      <td>1437</td>\n",
       "      <td>[{'genre': 'Casual, Free to Play, Indie, Simul...</td>\n",
       "      <td>40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$20.15</td>\n",
       "      <td>http://store.steampowered.com/bundle/1466/?utm...</td>\n",
       "      <td>$25.87</td>\n",
       "      <td>Naruto Shippuden Uncut Season 4, Vol. 1</td>\n",
       "      <td>1466</td>\n",
       "      <td>[{'genre': '', 'item_id': '527830', 'discounte...</td>\n",
       "      <td>22%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bundle_final_price                                         bundle_url  \\\n",
       "0             $66.46  http://store.steampowered.com/bundle/450/?utm_...   \n",
       "1             $20.15  http://store.steampowered.com/bundle/1473/?utm...   \n",
       "2             $20.15  http://store.steampowered.com/bundle/1474/?utm...   \n",
       "3             $15.17  http://store.steampowered.com/bundle/1437/?utm...   \n",
       "4             $20.15  http://store.steampowered.com/bundle/1466/?utm...   \n",
       "\n",
       "  bundle_price                              bundle_name bundle_id  \\\n",
       "0       $73.86             Dharker Studio 2015 Complete       450   \n",
       "1       $25.87  Naruto Shippuden Uncut Season 4, Vol. 3      1473   \n",
       "2       $25.87  Naruto Shippuden Uncut Season 4, Vol. 4      1474   \n",
       "3       $25.35      Pixel Puzzles Ultimate: Starter Kit      1437   \n",
       "4       $25.87  Naruto Shippuden Uncut Season 4, Vol. 1      1466   \n",
       "\n",
       "                                               items bundle_discount  \n",
       "0  [{'genre': 'Adventure, Indie, RPG', 'item_id':...             10%  \n",
       "1  [{'genre': '', 'item_id': '528360', 'discounte...             22%  \n",
       "2  [{'genre': '', 'item_id': '528380', 'discounte...             22%  \n",
       "3  [{'genre': 'Casual, Free to Play, Indie, Simul...             40%  \n",
       "4  [{'genre': '', 'item_id': '527830', 'discounte...             22%  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "with gzip.open('data/bundle_data.json.gz', 'rt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(ast.literal_eval(line))\n",
    "\n",
    "bundle_data = pd.DataFrame(rows)\n",
    "bundle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec17d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>items_count</th>\n",
       "      <th>steam_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76561197970982479</td>\n",
       "      <td>277</td>\n",
       "      <td>76561197970982479</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561197970...</td>\n",
       "      <td>[{'item_id': '10', 'item_name': 'Counter-Strik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>js41637</td>\n",
       "      <td>888</td>\n",
       "      <td>76561198035864385</td>\n",
       "      <td>http://steamcommunity.com/id/js41637</td>\n",
       "      <td>[{'item_id': '10', 'item_name': 'Counter-Strik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evcentric</td>\n",
       "      <td>137</td>\n",
       "      <td>76561198007712555</td>\n",
       "      <td>http://steamcommunity.com/id/evcentric</td>\n",
       "      <td>[{'item_id': '1200', 'item_name': 'Red Orchest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Riot-Punch</td>\n",
       "      <td>328</td>\n",
       "      <td>76561197963445855</td>\n",
       "      <td>http://steamcommunity.com/id/Riot-Punch</td>\n",
       "      <td>[{'item_id': '10', 'item_name': 'Counter-Strik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doctr</td>\n",
       "      <td>541</td>\n",
       "      <td>76561198002099482</td>\n",
       "      <td>http://steamcommunity.com/id/doctr</td>\n",
       "      <td>[{'item_id': '300', 'item_name': 'Day of Defea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id  items_count           steam_id  \\\n",
       "0  76561197970982479          277  76561197970982479   \n",
       "1            js41637          888  76561198035864385   \n",
       "2          evcentric          137  76561198007712555   \n",
       "3         Riot-Punch          328  76561197963445855   \n",
       "4              doctr          541  76561198002099482   \n",
       "\n",
       "                                            user_url  \\\n",
       "0  http://steamcommunity.com/profiles/76561197970...   \n",
       "1               http://steamcommunity.com/id/js41637   \n",
       "2             http://steamcommunity.com/id/evcentric   \n",
       "3            http://steamcommunity.com/id/Riot-Punch   \n",
       "4                 http://steamcommunity.com/id/doctr   \n",
       "\n",
       "                                               items  \n",
       "0  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
       "1  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
       "2  [{'item_id': '1200', 'item_name': 'Red Orchest...  \n",
       "3  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
       "4  [{'item_id': '300', 'item_name': 'Day of Defea...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "with gzip.open('data/australian_users_items.json.gz', 'rt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(ast.literal_eval(line)) \n",
    "\n",
    "rev_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286143e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.head()\n",
    "rev_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77b72e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m         rows.append(ast.literal_eval(line))\n\u001b[32m     10\u001b[39m df = pd.DataFrame(rows)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msteam_reviews.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pandas\\io\\parquet.py:79\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyArrowImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pandas\\io\\parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:174\u001b[39m\n\u001b[32m    167\u001b[39m     pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m         ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m     pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:166\u001b[39m, in \u001b[36mpatch_pyarrow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m         pickletools.dis(serialized, out)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    159\u001b[39m             _ERROR_MSG.format(\n\u001b[32m    160\u001b[39m                 storage_type=storage_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m             )\n\u001b[32m    164\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marrow.py_extension_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m     ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m )\n\u001b[32m    171\u001b[39m pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pyarrow\\types.pxi:2280\u001b[39m, in \u001b[36mpyarrow.lib.unregister_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "with gzip.open('data/steam_reviews.json.gz', 'rt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(ast.literal_eval(line))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df.to_parquet('steam_reviews.parquet', engine='pyarrow', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df.to_parquet('data/steam_reviews.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('steam_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d0d24f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>hours</th>\n",
       "      <th>products</th>\n",
       "      <th>product_id</th>\n",
       "      <th>page_order</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>early_access</th>\n",
       "      <th>page</th>\n",
       "      <th>found_funny</th>\n",
       "      <th>compensation</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chaos Syren</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>725280</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>This would not be acceptable as an entertainme...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>₮ʜᴇ Wᴀʀᴛᴏɴ</td>\n",
       "      <td>51.1</td>\n",
       "      <td>769.0</td>\n",
       "      <td>328100</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>looks like a facebook game</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello?&lt;</td>\n",
       "      <td>14.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>328100</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>Better than Minecraft</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Product received for free</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cyderine916</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35140</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>I love and idolized Batman and this game is Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DarklyThinking</td>\n",
       "      <td>16.6</td>\n",
       "      <td>577.0</td>\n",
       "      <td>35140</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>Still worth playing in 2018.\\nProbably my favo...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76561198007483075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         username  hours  products product_id  page_order        date  \\\n",
       "0     Chaos Syren    0.1      41.0     725280           0  2017-12-17   \n",
       "1      ₮ʜᴇ Wᴀʀᴛᴏɴ   51.1     769.0     328100           0  2017-12-27   \n",
       "2         hello?<   14.6       2.0     328100           1  2017-10-16   \n",
       "3     Cyderine916    5.0      64.0      35140           0  2018-01-04   \n",
       "4  DarklyThinking   16.6     577.0      35140           1  2018-01-04   \n",
       "\n",
       "                                                text  early_access  page  \\\n",
       "0  This would not be acceptable as an entertainme...         False     1   \n",
       "1                         looks like a facebook game         False     1   \n",
       "2                              Better than Minecraft         False     1   \n",
       "3  I love and idolized Batman and this game is Ma...         False     1   \n",
       "4  Still worth playing in 2018.\\nProbably my favo...         False     1   \n",
       "\n",
       "   found_funny               compensation            user_id  \n",
       "0          NaN                        NaN                NaN  \n",
       "1          NaN                        NaN                NaN  \n",
       "2          2.0  Product received for free                NaN  \n",
       "3          NaN                        NaN                NaN  \n",
       "4          NaN                        NaN  76561198007483075  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc3f89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative axis 0 index: -1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m user_codes = df[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m).cat.codes\n\u001b[32m     53\u001b[39m item_codes = df[\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m).cat.codes\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m R = \u001b[43msp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoo_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhours\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_codes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\scipy\\sparse\\_coo.py:101\u001b[39m, in \u001b[36m_coo_base.__init__\u001b[39m\u001b[34m(self, arg1, shape, dtype, copy, maxprint)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._shape) > \u001b[32m2\u001b[39m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mself\u001b[39m.coords = \u001b[38;5;28mtuple\u001b[39m(idx.astype(np.int64, copy=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coords)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danny\\anaconda3\\envs\\assign2\\Lib\\site-packages\\scipy\\sparse\\_coo.py:225\u001b[39m, in \u001b[36m_coo_base._check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33maxis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx.max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exceeds \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    223\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmatrix dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.shape[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx.min() < \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnegative axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx.min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: negative axis 0 index: -1"
     ]
    }
   ],
   "source": [
    "def mark_caps(word):\n",
    "    # Mark words that are all uppercase and longer than 1 character\n",
    "    if word.isupper() and len(word) > 1:\n",
    "        return word.lower() + \" <allcaps>\"\n",
    "    return word\n",
    "\n",
    "def clean_text(text, mark_allcaps=True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize any sequence of ♥ to a token\n",
    "    text = re.sub(r'[♥]+', ' <censored> ', text)\n",
    "\n",
    "    # Replace repeated punctuation / emojis with tokens\n",
    "    text = re.sub(r'!{2,}', ' <exclaim> ', text)   # !!! → <exclaim>\n",
    "    text = re.sub(r'\\?{2,}', ' <question> ', text) # ??? → <question>\n",
    "    text = re.sub(r':\\)', ' <smile> ', text)       # :) → <smile>\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Optionally mark all-caps words\n",
    "    if mark_allcaps:\n",
    "        text = ' '.join(mark_caps(w) for w in text.split())\n",
    "\n",
    "    # Lowercase everything (words that weren't all-caps are fine)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Allow letters, digits, common sentiment punctuation, +-/ and the token brackets <>\n",
    "    text = re.sub(r'[^a-z0-9<>\\s!\\?\\.,:\\+\\-/]', ' ', text)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "df['received_free'] = (df['compensation'] == 'Product received for free').astype(int)\n",
    "df = df.drop(columns=['compensation'])\n",
    "df['found_funny'] = df['found_funny'].fillna(0).astype(int)\n",
    "df['found_funny_log'] = np.log1p(df['found_funny'])\n",
    "df['hours'] = np.log1p(df['hours'])\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in critical columns (hours, products, date)\n",
    "df = df.dropna(subset=['hours', 'products', 'date'])\n",
    "df['user_id'] = df['user_id'].fillna('unknown_user')\n",
    "df['product_id'] = df['product_id'].fillna('unknown_product')\n",
    "# Extract Date Features\n",
    "df['review_month'] = df['date'].dt.month\n",
    "df['review_dow'] = df['date'].dt.dayofweek\n",
    "\n",
    "user_codes = df['user_id'].astype('category').cat.codes\n",
    "item_codes = df['product_id'].astype('category').cat.codes\n",
    "\n",
    "df.to_parquet('steam_reviews.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b6e8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['user_id', 'product_id'])\n",
    "user_codes = df['user_id'].astype('category').cat.codes\n",
    "item_codes = df['product_id'].astype('category').cat.codes\n",
    "\n",
    "R = sp.coo_matrix(\n",
    "    (df['hours'], (user_codes, item_codes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a54cda05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3158609"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# # -------------------------\n",
    "# # Text Features: TF-IDF\n",
    "# # -------------------------\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_text = tfidf.fit_transform(train_df['cleaned_text'])\n",
    "X_val_text = tfidf.transform(val_df['cleaned_text'])\n",
    "X_test_text = tfidf.transform(test_df['cleaned_text'])\n",
    "\n",
    "y_train = train_df['hours'].values\n",
    "y_val = val_df['hours'].values\n",
    "\n",
    "# -------------------------\n",
    "# Numeric & Categorical Features\n",
    "# -------------------------\n",
    "numeric_features = ['products', 'found_funny', 'review_month', 'review_dow', 'received_free']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_meta = preprocessor.fit_transform(train_df)\n",
    "X_val_meta = preprocessor.transform(val_df)\n",
    "X_test_meta = preprocessor.transform(test_df)\n",
    "\n",
    "# -------------------------\n",
    "# Combine text + meta\n",
    "# -------------------------\n",
    "X_train = sp.hstack([X_train_text, X_train_meta])\n",
    "X_val = sp.hstack([X_val_text, X_val_meta])\n",
    "X_test = sp.hstack([X_test_text, X_test_meta])\n",
    "\n",
    "# -------------------------\n",
    "# Train LinearSVR\n",
    "# -------------------------\n",
    "svr = LinearSVR(max_iter=5000, random_state=42)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = svr.predict(X_val)\n",
    "y_test_pred = svr.predict(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# Collaborative Filtering / Matrix Factorization\n",
    "# -------------------------\n",
    "# Create sparse user-item matrix\n",
    "user_codes = df['user_id'].astype('category').cat.codes\n",
    "item_codes = df['product_id'].astype('category').cat.codes\n",
    "\n",
    "R = sp.coo_matrix((df['hours'], (user_codes, item_codes)))\n",
    "\n",
    "# Use TruncatedSVD (approximate MF)\n",
    "n_factors = 50\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "user_factors = svd.fit_transform(R)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "# Predicted matrix: hours\n",
    "R_pred = user_factors @ item_factors.T\n",
    "\n",
    "# -------------------------\n",
    "# 8. EM / Probabilistic modeling (Gaussian Mixture on residuals)\n",
    "# -------------------------\n",
    "residuals = y_train - svr.predict(X_train)\n",
    "residuals = residuals.reshape(-1, 1)\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(residuals)\n",
    "\n",
    "# Example: probability of a residual belonging to each cluster\n",
    "probs = gmm.predict_proba(residuals)\n",
    "\n",
    "print(\"Pipeline ready. Shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Sparse user-item matrix R:\", R.shape)\n",
    "print(\"User factors:\", user_factors.shape)\n",
    "print(\"Item factors:\", item_factors.shape)\n",
    "print(\"GMM probabilities:\", probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Train/Validation/Test split\n",
    "# -------------------------\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "y_train = train_df['hours'].values\n",
    "y_val = val_df['hours'].values\n",
    "y_test = test_df['hours'].values\n",
    "\n",
    "# -------------------------\n",
    "# 2. Text Features: TF-IDF\n",
    "# -------------------------\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_text = tfidf.fit_transform(train_df['cleaned_text'])\n",
    "X_val_text = tfidf.transform(val_df['cleaned_text'])\n",
    "X_test_text = tfidf.transform(test_df['cleaned_text'])\n",
    "\n",
    "# -------------------------\n",
    "# 3. Numeric Features\n",
    "# -------------------------\n",
    "numeric_features = ['products', 'found_funny', 'review_month', 'review_dow', 'received_free']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_meta = preprocessor.fit_transform(train_df)\n",
    "X_val_meta = preprocessor.transform(val_df)\n",
    "X_test_meta = preprocessor.transform(test_df)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Combine text + meta\n",
    "# -------------------------\n",
    "X_train = sp.hstack([X_train_text, X_train_meta])\n",
    "X_val = sp.hstack([X_val_text, X_val_meta])\n",
    "X_test = sp.hstack([X_test_text, X_test_meta])\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train LinearSVR\n",
    "# -------------------------\n",
    "svr = LinearSVR(max_iter=5000, random_state=42)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_train_svr = svr.predict(X_train)\n",
    "y_val_svr = svr.predict(X_val)\n",
    "y_test_svr = svr.predict(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Collaborative Filtering / Matrix Factorization\n",
    "# -------------------------\n",
    "user_codes = df['user_id'].astype('category').cat.codes\n",
    "item_codes = df['product_id'].astype('category').cat.codes\n",
    "\n",
    "R = sp.coo_matrix((df['hours'], (user_codes, item_codes)))\n",
    "\n",
    "n_factors = 50\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "user_factors = svd.fit_transform(R)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "R_pred = user_factors @ item_factors.T\n",
    "\n",
    "# Map MF predictions to train/val/test users/items\n",
    "user_map = dict(zip(df['user_id'], user_codes))\n",
    "item_map = dict(zip(df['product_id'], item_codes))\n",
    "\n",
    "def get_mf_pred(df_subset):\n",
    "    u_idx = df_subset['user_id'].map(user_map).values\n",
    "    i_idx = df_subset['product_id'].map(item_map).values\n",
    "    return R_pred[u_idx, i_idx]\n",
    "\n",
    "y_train_mf = get_mf_pred(train_df)\n",
    "y_val_mf = get_mf_pred(val_df)\n",
    "y_test_mf = get_mf_pred(test_df)\n",
    "\n",
    "# -------------------------\n",
    "# 7. EM / Gaussian Mixture on residuals\n",
    "# -------------------------\n",
    "residuals = y_train - y_train_svr\n",
    "residuals = residuals.reshape(-1, 1)\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(residuals)\n",
    "cluster_means = gmm.means_.flatten()\n",
    "\n",
    "# -------------------------\n",
    "# 8. Combine SVR + MF + GMM residual adjustment\n",
    "# -------------------------\n",
    "def combined_prediction(svr_pred, mf_pred, gmm, cluster_means, alpha=0.5):\n",
    "    # residuals are unknown at inference, approximate with zeros\n",
    "    residuals = np.zeros_like(svr_pred).reshape(-1, 1)\n",
    "    probs = gmm.predict_proba(residuals)\n",
    "    expected_residual = np.dot(probs, cluster_means)\n",
    "    combined = alpha * svr_pred + (1 - alpha) * mf_pred + expected_residual\n",
    "    return combined\n",
    "\n",
    "y_val_combined = combined_prediction(y_val_svr, y_val_mf, gmm, cluster_means, alpha=0.5)\n",
    "y_test_combined = combined_prediction(y_test_svr, y_test_mf, gmm, cluster_means, alpha=0.5)\n",
    "\n",
    "# -------------------------\n",
    "# 9. Evaluation\n",
    "# -------------------------\n",
    "def evaluate(y_true, y_pred, name=\"\"):\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name} RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "evaluate(y_val, y_val_combined, \"Validation\")\n",
    "evaluate(y_test, y_test_combined, \"Test\")\n",
    "\n",
    "print(\"Pipeline ready. Shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Sparse user-item matrix R:\", R.shape)\n",
    "print(\"User factors:\", user_factors.shape)\n",
    "print(\"Item factors:\", item_factors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbeeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 0. Text cleaning\n",
    "# -------------------------\n",
    "def mark_caps(word):\n",
    "    if word.isupper() and len(word) > 1:\n",
    "        return word.lower() + \" <allcaps>\"\n",
    "    return word\n",
    "\n",
    "def clean_text(text, mark_allcaps=True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[♥]+', ' <censored> ', text)\n",
    "    text = re.sub(r'!{2,}', ' <exclaim> ', text)\n",
    "    text = re.sub(r'\\?{2,}', ' <question> ', text)\n",
    "    text = re.sub(r':\\)', ' <smile> ', text)\n",
    "    if mark_allcaps:\n",
    "        text = ' '.join(mark_caps(w) for w in text.split())\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9<>\\s!\\?\\.,:\\+\\-/]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# -------------------------\n",
    "# 1. Preprocess dataframe\n",
    "# -------------------------\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "df['received_free'] = (df['compensation'] == 'Product received for free').astype(int)\n",
    "df['found_funny'] = df['found_funny'].fillna(0).astype(int)\n",
    "df['hours'] = np.log1p(df['hours'])\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "df = df.drop(columns=['compensation'])\n",
    "df = df.dropna(subset=['hours', 'products', 'date'])\n",
    "df['user_id'] = df['user_id'].fillna('unknown_user')\n",
    "df['product_id'] = df['product_id'].fillna('unknown_product')\n",
    "\n",
    "df['review_month'] = df['date'].dt.month\n",
    "df['review_dow'] = df['date'].dt.dayofweek\n",
    "\n",
    "user_codes = df['user_id'].astype('category').cat.codes\n",
    "item_codes = df['product_id'].astype('category').cat.codes\n",
    "\n",
    "# -------------------------\n",
    "# 2. Train/Validation/Test split\n",
    "# -------------------------\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "y_train = train_df['hours'].values\n",
    "y_val = val_df['hours'].values\n",
    "y_test = test_df['hours'].values\n",
    "\n",
    "# -------------------------\n",
    "# 3. Text Features: TF-IDF\n",
    "# -------------------------\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_text = tfidf.fit_transform(train_df['cleaned_text'])\n",
    "X_val_text = tfidf.transform(val_df['cleaned_text'])\n",
    "X_test_text = tfidf.transform(test_df['cleaned_text'])\n",
    "\n",
    "# -------------------------\n",
    "# 4. Numeric Features\n",
    "# -------------------------\n",
    "numeric_features = ['products', 'found_funny', 'review_month', 'review_dow', 'received_free']\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_features)\n",
    "])\n",
    "\n",
    "X_train_meta = preprocessor.fit_transform(train_df)\n",
    "X_val_meta = preprocessor.transform(val_df)\n",
    "X_test_meta = preprocessor.transform(test_df)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Combine text + meta\n",
    "# -------------------------\n",
    "X_train = sp.hstack([X_train_text, X_train_meta])\n",
    "X_val = sp.hstack([X_val_text, X_val_meta])\n",
    "X_test = sp.hstack([X_test_text, X_test_meta])\n",
    "\n",
    "# -------------------------\n",
    "# 6. Train LinearSVR\n",
    "# -------------------------\n",
    "svr = LinearSVR(max_iter=5000, random_state=42)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_train_svr = svr.predict(X_train)\n",
    "y_val_svr = svr.predict(X_val)\n",
    "y_test_svr = svr.predict(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Collaborative Filtering / MF\n",
    "# -------------------------\n",
    "R = sp.coo_matrix((df['hours'], (user_codes, item_codes)))\n",
    "\n",
    "# Use MiniBatchTruncatedSVD for speed on large sparse matrix\n",
    "n_factors = 50\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "user_factors = svd.fit_transform(R)\n",
    "item_factors = svd.components_.T\n",
    "R_pred = user_factors @ item_factors.T\n",
    "\n",
    "# Map MF predictions\n",
    "user_map = dict(zip(df['user_id'], user_codes))\n",
    "item_map = dict(zip(df['product_id'], item_codes))\n",
    "\n",
    "def get_mf_pred(df_subset):\n",
    "    u_idx = df_subset['user_id'].map(user_map).values\n",
    "    i_idx = df_subset['product_id'].map(item_map).values\n",
    "    return R_pred[u_idx, i_idx]\n",
    "\n",
    "y_train_mf = get_mf_pred(train_df)\n",
    "y_val_mf = get_mf_pred(val_df)\n",
    "y_test_mf = get_mf_pred(test_df)\n",
    "\n",
    "# -------------------------\n",
    "# 8. EM / Gaussian Mixture on residuals\n",
    "# -------------------------\n",
    "residuals = y_train - y_train_svr\n",
    "residuals = residuals.reshape(-1, 1)\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(residuals)\n",
    "cluster_means = gmm.means_.flatten()\n",
    "\n",
    "# -------------------------\n",
    "# 9. Combine SVR + MF + GMM residuals\n",
    "# -------------------------\n",
    "def combined_prediction(svr_pred, mf_pred, gmm, cluster_means, alpha=0.5):\n",
    "    residuals = np.zeros_like(svr_pred).reshape(-1, 1)\n",
    "    probs = gmm.predict_proba(residuals)\n",
    "    expected_residual = np.dot(probs, cluster_means)\n",
    "    combined = alpha * svr_pred + (1 - alpha) * mf_pred + expected_residual\n",
    "    return combined\n",
    "\n",
    "y_val_combined = combined_prediction(y_val_svr, y_val_mf, gmm, cluster_means)\n",
    "y_test_combined = combined_prediction(y_test_svr, y_test_mf, gmm, cluster_means)\n",
    "\n",
    "# -------------------------\n",
    "# 10. Evaluation\n",
    "# -------------------------\n",
    "def evaluate(y_true, y_pred, name=\"\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name} RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "evaluate(y_val, y_val_combined, \"Validation\")\n",
    "evaluate(y_test, y_test_combined, \"Test\")\n",
    "\n",
    "print(\"Pipeline ready.\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Sparse user-item matrix R:\", R.shape)\n",
    "print(\"User factors:\", user_factors.shape)\n",
    "print(\"Item factors:\", item_factors.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
